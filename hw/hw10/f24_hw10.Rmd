---
title: "Homework 10"
author: "your name here"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T,eval=T,message=F,warning=F,fig.align='center')
library(tidyverse)
```

## Problem 1) More regression with `mtcars` (6 points, 1pt each)

In lecture, we worked briefly with the `mtcars` data set.
Let's get more regression practice by working with it some more.

### a) background

Run `?mtcars` in the console (please __do not__ add it to this `Rmd` file) and briefly read the help page.
Specifically, take note of the following:

1. What is the source of this data?

1974 Motor Trend magazine, fuel consumption and aspects of automobile design and performance.

2. What is this data set measuring (i.e., what was the response variable in the original study, at least based on the brief description in the R documentation)?

Fuel consumption in mpg.

3. What predictors are available and what do they mean?

cyl	Number of cylinders

disp	Displacement (cu.in.)

hp	Gross horsepower

drat	Rear axle ratio

wt	Weight (1000 lbs)

qsec	1/4 mile time

vs	Engine (0 = V-shaped, 1 = straight)

am	Transmission (0 = automatic, 1 = manual)

gear	Number of forward gears

carb	Number of carburetors

You may want to also run `head(mtcars, 10)` or `View(mtcars)` to inspect the data frame briefly before moving on.

### b) Fitting a model

Use `lm` to run a regression of `mpg` on a few predictors in the data frame (choose two or three that you think would make a good model -- don't use all ten; we'll talk about why in later lectures). Save your fitted model as an object called `lm.mtcars`.
Make sure to include `data = mtcars` as a keyword argument to `lm` so that R knows what data frame to use.

```{r}
lm.mtcars = lm(mpg ~ hp + wt, data = mtcars)
plot(lm.mtcars,ask=F,which=1:2)
```

Briefly inspect the residuals plot by running `plot(lm.mtcars,ask=F,which=1:2)`.
What do you observe, and what does it mean?

it seems there's a curvature in the residuals, meaning the data right not be represented by a linear relationship.

The tails of the data seem to diverge from the line, indicating the residuals may not be normally distributed.

### c) Interpreting the model

View the summary of your model by uncommenting and running the code below.
```{r}
summary(lm.mtcars)
```

Pick one of your predictors and give an interpretation of the estimate and standard error for its coefficient.

Choose hp. 

Keeping all other factors constant, a unit increase in hp will decrease the mpg of the car by 0.03177.

Standard error of 0.00903 means if we keep drawing new samples of cars, the distribution of hp coefficients has standard deviation of 0.00903. 

Be careful in your wording of the interpretation.

Which coefficients are statistically significantly different from zero? How do you know?

hp, intercept, and wt are both statistically significant because they have p values under $.05$.

### d) Interpreting residuals

What is the Residual Standard Error (RSE) for this model? How many degrees of freedom does it have?

RSE is 2.593 on 29 DF.

What is the value of $R^2$ for this model? (__Hint:__ look at the output of `summary`) Give an interpretation of this value.

.8268, so the model could explain around 83% of the variance in the data. 

### e) Adjusted $R^2$

Briefly read about the adjusted $R^2$ [here](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/adjusted-r2/).
What is the adjusted $R^2$ of this model and how does this differ from the usual $R^2$ value? (__Hint:__ again, look at the output of `summary`).

The adjusted $R^2$ is .8148, meaning the model could explain around 81% of variance by adjusting for the number of predictors. 

### f) CIs for coefficients

Read the documentation for the `confint` function, and use it to generate $95\%$ confidence intervals for the coefficients of your model.
Give an interpretation of these confidence intervals.

```{r}
confint(lm.mtcars)
```

We are 95% confident that:

The intercept for the model will be between 33.95738245 to 40.49715778.

Holding wt constant, a unit increase in hp will decrease the MPG by between 0.05024078 to 0.01330512.

Holding hp constant, a unit increase in wt will decrease the MPG by between -5.17191604 to -2.58374544.

## Problem 2) the `cats` data set (8 points; 2pt each)

The `cats` data set, included in the `MASS` library, contains data recorded from 144 cats.
Each row of the data set contains the body weight (`Bwt`, in kgs), heart weight (`Hwt`, in grams) and the sex (`Sex`, levels `'F'` and `'M'`) for one of the cats in the data set.

```{r}
library(MASS)
head(cats)
```

### a) plotting the data

Create a scatter plot showing heart weight on the y-axis and body weight on the x-axis.
Ignore the `Sex` variable in this plot.


```{r}
plot(cats$Bwt, cats$Hwt)
```

Briefly describe what you see. Is there a clear trend in the data?

There seems to be a clear linear trend in the data. At least, these 2 variables are correlated.

### b) fitting a linear model

Fit a linear regression model to predict cat heart weight from cat body weight (and using an intercept term, of course).

```{r}
lm.cats = lm(cats$Hwt ~ cats$Bwt)
summary(lm.cats)
```

Examine the coefficients of your fitted model.
What is the coefficient for the `Bwt` variable? 4.0341

Interpret this coefficient-- a unit change in body weight yields how much change in heart weight?

A unit change in body weight will result in a 4.0341 change in heart weight on average. 

### c) back to plotting

Create the same plot from Part a above, but this time color the points in the scatter plot according to the `Sex` variable.
You may use either `ggplot2` or the built-in R plotting tools, though I would recommend the former, for this.

```{r}
cats %>% ggplot(aes(x = Bwt, y = Hwt, color = Sex)) + geom_point() 
```

You should see a clear pattern. Describe it. A sentence or two is fine here.

When $Bwt > 2.5$ the data appears to be dominated by males. It also appears the linear trend is more pronounced after this point. 

### d) adding `Sex` and an interaction

From looking at the data, it should be clear that the `Sex` variable has explanatory power in predicting heart weight, but it is also very correlated with body weight.

Fit a new linear regression model, still predicting heart weight, but this time including both body weight and sex as predictors *and* an interaction term between body weight and sex.
Take note of how R assigns `Sex` a dummy encoding.

```{r}
lm.cats = lm(cats$Hwt ~ cats$Bwt + cats$Sex + cats$Bwt:cats$Sex)
summary(lm.cats)
```

Examine the outputs of your model.
In particular, note the coefficients of `Sex` and the interaction between `Bwt` and `Sex`.
Are both of these coefficients statistically significantly different from zero?

Yes they are both statistically significantly different from 0 with $p < .05$.

How do you interpret the interaction term?

If a cat is male then the coefficient for Bwt will increase by 1.6763.



## Problem 3) Quantitative and Categorical Predictors (8 points)

(This problem is based on a plot from [The behavior of different clays subjected to a fast-drying cycle for traditional ceramic manufacturing](https://doi.org/10.1016/j.jksues.2022.05.003), but the data has been simulated)

An experiment was conducted on three types of **material**: Plastic clay, Sandy clay and claystone. The **plastic deformation** (\%) was measured at various **moisture** (\%) levels. Data is found in the `clay_sample.csv` file. Plastic deformation is denoted `pd` in the dataset.

### a) Fit the model (4 points)
Create a linear model model predicting plastic deformation from moisture. Include material and the interaction between material and moisture. 


```{r}
clay = read.csv("clay_sample.csv")
head(clay)
unique(clay$ material)
lm.clay = lm(clay$pd ~ clay$moisture * clay$material)
summary(lm.clay)
```

a.i. Output the standard model summary using `summary()`. 
  
a.ii. Interpret the $R^2$$ statistic value in one sentence.

The model is able to capture ~88% of variance in the data.

a.iii. Which material represents the baseline?

The baseline is claystone. 

a.iv. Explain how the residual standard error degrees of freedom is related to sample size and the model size.

Residual standard error is the standard error of the models residuals. The formula for RSE is given by: 

$$\sqrt{\frac{\sum_i^n{(y_i - \hat{y})^2}}{df}}$$
The formula for df is given by $df = n - p - 1$, where $n$ is the sample size and $p$ is the model size. Then substitute back in the RSE formula:  

$$\sqrt{\frac{\sum_i^n{(y_i - \hat{y})^2}}{n - p - 1}}$$

At a glance, we can decrease the RSE by increasing the sample size or increasing model size (assuming this has no effect on the residuals).

### b) Interpret coefficient 1 (1 pt)
Provide an interpretation for the value of the coefficient of moisture, and comment on its significance.

If the material is claystone, then holding all other factors constant, a unit increase in moisture will lead to a 7.7928 increase in plastic deformation. This coefficient is statistically significant, with $p < .05$ and reject the null hypothesis the coefficient $= 0$.

### c) Interpret coefficient 2 (1 pt)
Provide an interpretation for the value of the coefficient of one of the materials, and comment on its significance.

If the material is plastic clay, then there will be a flat 23.9557 increase in plastic deformation compared to claystone. This coefficient is not statistically significant, with $p > .05$ so we accept the null hypothesis that this coefficient $= 0$.

### d) Interpret coefficient 3 (1 pt)
Provide an interpretation for the value of the coefficient of one of the interaction terms, and comment on its significance.

If the material is plastic clay, then the coefficient for moisture will decrease by -3.595 compared to claystone. This coefficient is statistically significant, with $p < .05$ and reject the null hypothesis the coefficient $= 0$.

### e) Assess assumptions (1 pt)
Finally display the first two diagnostic plots using `plot(<YOUR MODEL>, which=1:2)` and discuss three of the assumptions of the linear model: normality of error term, linearity and the relationship, and constant variance of the error term. You do not have to do any more modeling after this step, but comment about what you *might* want to do to improve the model.

```{r}
plot(lm.clay, which=1:2)
```

There is a fanning out of residuals, indicating there is not constant variance of the error term. There also appears to be a nonlinear relationship in the data, indicated by the curved line in the 1st plot. It appears the error term is largely normally distributed by the QQ plot, except for the tails.

To improve the model we might consider adding a nonlinear term in the classifier to capture the nonlinear patterns in the data. 

## Problem 4) Using Multiple regression to fit nonlinear data (8 points; 2 pt each)

Open the dataset `multData.csv`. This data set consists of three predictor variables, simply named `X1`, `X2` and `X3`. The response variable is `Y`. In this problem you will explore how to use the multiple regression model to model nonlinear relationships.

### a) the first model

First we will explore the relationship between $Y$ and the first two predictors $X1$ and $X2$. Fit the linear model

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$
Interpret the coefficients of both X1 and X2. 

```{r}
mult_data = read.csv("multData.csv")
lm.mult = lm(Y ~ X1 + X2, data = mult_data)
summary(lm.mult)
```

All coefficients are significant with $p < .05$, i.e all coefficients $\neq 0$.

For the intercept, With $X_1 = 0$ and $X_2 = 0$, $Y = 948.5316$. 

Holding $X_2$ constant, a unit increase in $X_1$ will decrease $Y$ by $-12.6318$.

Holding $X_1$ constant, a unit increase in $X_2$ will decrease $Y$ by $-35.1420$.

### b) Investigating interaction of quantitative predictors

Next introduce an interaction term to the model
$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2 + \epsilon$$

Fit the model and view the summary output. Has this improved the model fit? Did anything surprising happen to the coefficients? Try to explain what happened.

```{r}
mult_data = read.csv("multData.csv")
lm.mult = lm(Y ~ X1 * X2, data = mult_data)
summary(lm.mult)
```

Has the model fit improved? In what way (Justify your answer)? 

There seems to be a marginal improvement in RSE, $R^2$ and adjusted $R^2$. So it does seem the model improved fit.

The intercept has become much smaller, and the coefficients for $X_1$ and $X_2$ are now positive. 

Recall that the coefficient for the interaction term will change the coefficients for $X_1$ and $X_2$. Since the coefficient for the interaction term is negative, we no longer need negative coefficients for $X_1$ and $X_2$ because for large enough $X_1$ or $X_2$ the coefficient will become negative. This happens because the effect of one predictor depends on the other.

### c) Introducing the last predictor

Next fit a model that introduces the `X3` variable. 

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2  + \beta_4 X_3 \epsilon$$

```{r}
mult_data = read.csv("multData.csv")
lm.mult = lm(Y ~ X1 * X2 + X3, data = mult_data)
summary(lm.mult)
plot(lm.mult, which = 1:2)
```

Has the model fit improved? In what way (Justify your answer)? 

Yes the model fit has improved in every way. It can explain nearly all the variance in the data and the residual standard error is around 9 times less than of previous models.

### d) Considering higher order terms

Finally explore higher order terms for the X3 variable: Introduce $X3^2$, $X3^3$ etc and determine if any of these higher order terms are justified in the model. Explain your reasoning and present your final model. Look at the diagnostic plots and discuss whether the assumptions of the multiple regression model seem to be justified.

```{r}
mult_data = read.csv("multData.csv")
X_3_pow = (mult_data$X3 ^ 2)
lm.mult = lm(Y ~ X1 * X2 + X_3_pow, data = mult_data)
summary(lm.mult)
plot(lm.mult, which = 1:2)
```

For this model the residuals have zero mean and constant variance, but they are not normally distributed.

```{r}
X_3_pow = (mult_data$X3 ^ 3)
lm.mult = lm(Y ~ X1 * X2 + X_3_pow, data = mult_data)
summary(lm.mult)
plot(lm.mult, which = 1:2)
```

For this model the residuals have zero mean and constant variance, but they are not normally distributed.

It does not appear the assumptions of multiple regression are justified. The residuals are not normally distributed and we should not include this higher order term.