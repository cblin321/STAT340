---
title: "Homework 7"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T,eval=T,message=F,warning=F,fig.align='center')
library(tidyverse)
```

## Problem \#1: Estimating Quantiles <small>(8 pts; 2pts each)</small>

There are 9 algorithms in R to estimate population quantiles. Type `?quantile` to read about them. Here we will investigate the variance of some of these estimators. To use the quantile function you use the syntax
`quantile(vector, probs, type)`.
For example if you have data in a vector called `sampleData` and you wish to estimate the 80th percentile using algorithm 7 (the default), you use
`quantile(sampleData, .80, type=7)`

Suppose we're interested in the 95th percentile for $X$, and we know that $X$ follows a uniform distribution. We want to randomly sample $n=30$ values and estimate the 95th percentile. Using MC simulation estimate the following:

a. Which quantile algorithm (4 through 9) has the smallest absolute bias ($|\hat{\theta}-\theta|$)? *Hint: you can use $unif(0,1)$ for the purposes of this estimation, as your answer won't depend on the upper and lower bounds chosen.*

For $\mathcal{U}[0, 1]$ we know the 95th percentile to truly be $.95$:

```{r}
set.seed(123) 
starting_sample = runif(30, 0, 1)
point_est = quantile(starting_sample, .95, type = 4)
mean_bias = c()
for (type in 4:9) {
  bias = c()
  for (i in 1:1e3) {
    sample_data = runif(30, 0, 1)
    point_est_mc = quantile(sample_data, .95, type = type, names = FALSE)
    bias = append(bias, abs(.95 - point_est_mc))
  }
  print(paste("bias for:", type))
  print(mean(bias))
  mean_bias = append(mean_bias, mean(bias))
}
print(paste("smallest bias:", min(mean_bias)))
```

b. Which quantile algorithm (4 through 9) has the smallest variance?

```{r}
set.seed(123) 
starting_sample = runif(30, 0, 1)
point_est = quantile(starting_sample, .95, type = 4)
percentile = c()
mean_var = c()
for (type in 4:9) {
  percentile = c()
  for (i in 1:1e3) {
    sample_data = runif(30, 0, 1)
    point_est_mc = quantile(sample_data, .95, type = type, names = FALSE)
    percentile = append(percentile, point_est_mc)
  }
  print(paste("variance for:", type))
  print(var(percentile))
  mean_var = append(mean_var, var(percentile))
}
print(paste("smallest var:", min(mean_var)))
```

c. Which method is best for estimating the 95th percentile from a uniform distribution? Justify your answer.

The least biased estimator is 6, and the most consistent estimator is 6. From these statistics it seems plausible 6 is the most accurate and most consistent estimator, making it the best here for the uniform.

d. What about if $X\sim N(\mu, \sigma^2)$? Would you prefer a different method for estimating the 95th percentile from a normal distribution? *Hint: repeat the same analysis for $N(0,1)$.*

```{r}
set.seed(123) 
starting_sample = runif(30, 0, 1)
point_est = quantile(starting_sample, .95, type = 4)
percentile = c()
mean_var = c()
for (type in 4:9) {
  percentile = c()
  for (i in 1:1e3) {
    sample_data = rnorm(30, 0, 1)
    point_est_mc = quantile(sample_data, .95, type = type, names = FALSE)
    percentile = append(percentile, point_est_mc)
  }
  print(paste("variance for:", type))
  print(var(percentile))
  mean_var = append(mean_var, var(percentile))
}
print(paste("smallest var:", min(mean_var)))
```

```{r}
set.seed(123) 
starting_sample = runif(30, 0, 1)
point_est = quantile(starting_sample, .95, type = 4)
mean_bias = c()
for (type in 4:9) {
  bias = c()
  for (i in 1:1e3) {
    sample_data = rnorm(30, 0, 1)
    point_est_mc = quantile(sample_data, .95, type = type, names = FALSE)
    bias = append(bias, abs(.95 - point_est_mc))
  }
  print(paste("bias for:", type))
  print(mean(bias))
  mean_bias = append(mean_bias, mean(bias))
}
print(paste("smallest bias:", min(mean_bias)))
```

We can see that 7 is the most consistent estimator and 4 is the least biased when estimating from the normal. Depending on the use case of the estimator, either is preferable over 6.

## Problem \#2: Estimating a Geometric $p$ <small>(6 pts; 2 pts each)</small>

a. Use the method of moments to come up with an estimator for a geometric distributions parameter $p$. *Hint: Use the fact that if $X\sim Geom(p)$ then $EX=\frac{1-p}{p}*$. 

$EX=\frac{1-p}{p} = \frac{1}{p} - 1$, then it's each to see that $(EX + 1)^{-1} = p$.



b. Estimate the sampling distribution of this estimator when we sample $n=13$ values from from $Geom(.15)$. Show the histogram of the estimated sampling distribution.

```{r}
reps = replicate(1e4, 1 / (1 + mean(rgeom(13, .15))))
hist(reps)
```

c. Estimate the bias of this estimator. Is it biased? If it is biased how would you modify it so that you could create an unbiased estimator?

```{r}
hist(abs(.15 - reps))
mean(abs(.15 - reps))

```



## Problem \#3: Estimating $\lambda$ from a Poisson Distribution<small>(8 pts; 2 pts each)</small>

It is interesting that if $X\sim Pois(\lambda)$ that $EX=VarX=\lambda$. One could use either $\bar{X}$ or $S^2$ as an estimator of $\lambda$ perhaps. 

a. Using $n=15$ and $\lambda=20$ for this problem, use MC simulation to estimate the sampling distribution of The estimator $\bar{X}$. Show its histogram. 
b. Repeat the same but this time use $S^2$. 
c. Compare the two estimators. Would you prefer one over the other? Why?
d. What about a linear combination of the two variables? Could you construct an estimator of $\lambda$ of the form $a\bar{X} + bS^2$ that would be better than using either of them by themselves? 


## Problem \#4: The Standard Error of $\bar{X}$<small>(8 pts; 2 pts each)</small>

What would be the required sample size $n$ so that the standard error of $\bar{X}$ (i.e. $SD(\bar{X})$) would be 2 (or just under 2) for the following populations:

a. $Normal(1000, 10^2)$
b. $Pois(75)$
c. $Binom(200, .35)$
d. $exp(.05)$



